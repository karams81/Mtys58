import requests
import re
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  AtomSporTV  â€“  CanlÄ± MaÃ§ + TV KanallarÄ± M3U
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
START_URL    = "https://url24.link/AtomSporTV"
MATCHES_URL  = "https://teletv3.top/load/matches.php"
LOGO_BASE    = "https://im.mackolik.com/img/logo/buyuk"
OUTPUT_FILE  = "atom_mac.m3u"

GREEN  = "\033[92m"
YELLOW = "\033[93m"
RESET  = "\033[0m"

headers = {
    'Accept': '*/*',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'tr-TR,tr;q=0.8',
    'Connection': 'keep-alive',
    'User-Agent': 'Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/138.0.0.0 Mobile Safari/537.36',
    'Referer': 'https://url24.link/'
}

# â”€â”€ TV KanallarÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TV_CHANNELS = [
    # (slug-id, GÃ¶rÃ¼nen Ad, logo_url)
    ("bein-sports-1", "BEIN SPORTS 1",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/BeIN_Sports_1_HD.svg/200px-BeIN_Sports_1_HD.svg.png"),
    ("bein-sports-2", "BEIN SPORTS 2",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/BeIN_Sports_2_HD.svg/200px-BeIN_Sports_2_HD.svg.png"),
    ("bein-sports-3", "BEIN SPORTS 3",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/BeIN_Sports_3_HD.svg/200px-BeIN_Sports_3_HD.svg.png"),
    ("bein-sports-4", "BEIN SPORTS 4",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/BeIN_Sports_4_HD.svg/200px-BeIN_Sports_4_HD.svg.png"),
    ("s-sport",       "S SPORT",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/S_Sport_logo.svg/200px-S_Sport_logo.svg.png"),
    ("s-sport-2",     "S SPORT 2",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/S_Sport_logo.svg/200px-S_Sport_logo.svg.png"),
    ("tivibu-spor-1", "TÄ°VÄ°BU SPOR 1", ""),
    ("tivibu-spor-2", "TÄ°VÄ°BU SPOR 2", ""),
    ("tivibu-spor-3", "TÄ°VÄ°BU SPOR 3", ""),
    ("trt-spor",      "TRT SPOR",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/TRT_Spor_logo.svg/200px-TRT_Spor_logo.svg.png"),
    ("trt-yildiz",    "TRT YILDIZ", ""),
    ("trt1",          "TRT 1",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/TRT_1_logo.svg/200px-TRT_1_logo.svg.png"),
    ("aspor",         "ASPOR",
     "https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/A_Spor_logo.svg/200px-A_Spor_logo.svg.png"),
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_base_domain():
    """Redirect zincirini takip ederek ana domain'i bul."""
    try:
        r1 = requests.get(START_URL, headers=headers, allow_redirects=False, timeout=10)
        if 'location' in r1.headers:
            r2 = requests.get(r1.headers['location'], headers=headers,
                              allow_redirects=False, timeout=10)
            if 'location' in r2.headers:
                domain = r2.headers['location'].strip().rstrip('/')
                print(f"Ana Domain : {domain}")
                return domain
    except Exception as e:
        print(f"Domain hatasÄ±: {e}")
    return "https://atomsportv489.top"


def normalize_logo(src):
    """GÃ¶reli / protokolsÃ¼z logo URL'lerini tam URL'ye Ã§evir."""
    if not src:
        return ""
    if src.startswith("http"):
        return src
    if src.startswith("//"):
        return "https:" + src
    # GÃ¶receli yol â†’ mackolik logo base'i
    return LOGO_BASE + "/" + src.lstrip("/")


def get_matches():
    """matches.php'den maÃ§ listesini parse et; takÄ±m logolarÄ±nÄ± da Ã§ek."""
    print(f"MaÃ§lar Ã§ekiliyor â†’ {MATCHES_URL}")
    try:
        resp = requests.get(MATCHES_URL, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')

        matches = []
        skip_words = {'futbol', 'futbol tr', 'futboi', 'gÃ¼nÃ¼n maÃ§Ä±', 'futbol'}

        for a in soup.find_all('a', href=True):
            href = a['href']
            mid = re.search(r'matches\?id=([a-f0-9]+)', href)
            if not mid:
                continue
            match_id = mid.group(1)

            # Logolar
            imgs = a.find_all('img')
            home_logo = normalize_logo(imgs[0]['src']) if len(imgs) >= 1 else ""
            away_logo = normalize_logo(imgs[1]['src']) if len(imgs) >= 2 else ""
            logo = home_logo or away_logo

            # Metin
            lines = [l.strip() for l in a.get_text('\n').splitlines()
                     if l.strip() and l.strip().lower() not in skip_words]

            saat, lig, home_team, away_team = '', '', '', ''
            for line in lines:
                if '|' in line and not saat:
                    parts = line.split('|', 1)
                    saat = parts[0].strip()
                    lig  = parts[1].strip()
                elif saat and not home_team:
                    home_team = line
                elif saat and home_team and not away_team:
                    away_team = line

            home_team = home_team or "Ev Sahibi"
            away_team = away_team or "Deplasman"
            name = f"{saat} {home_team} - {away_team}"

            matches.append({
                'id'       : match_id,
                'name'     : name,
                'home'     : home_team,
                'away'     : away_team,
                'home_logo': home_logo,
                'away_logo': away_logo,
                'logo'     : logo,
                'time'     : saat,
                'league'   : lig,
                'group'    : lig if lig else 'MaÃ§lar',
            })
            print(f"  âš½ [{saat}] {home_team} vs {away_team}  (id={match_id[:8]}â€¦)  logo={bool(logo)}")

        print(f"\nToplam {len(matches)} maÃ§ bulundu.")
        return matches

    except Exception as e:
        print(f"MaÃ§ Ã§ekme hatasÄ±: {e}")
        return []


def get_m3u8(resource_id, base_domain):
    """Verilen ID iÃ§in m3u8 stream URL'sini Ã§ek."""
    try:
        h = headers.copy()
        h['Referer'] = base_domain

        resp = requests.get(f"{base_domain}/matches?id={resource_id}", headers=h, timeout=10)
        fetch_m = re.search(r'fetch\(\s*["\']([^"\']+)["\']', resp.text)
        if not fetch_m:
            return None

        fetch_url = fetch_m.group(1).strip()
        if not fetch_url.endswith(resource_id):
            fetch_url += resource_id

        h['Origin'] = base_domain
        resp2 = requests.get(fetch_url, headers=h, timeout=10)
        data  = resp2.text

        for pat in [
            r'"deismackanal":"(.*?)"',
            r'"stream":\s*"(.*?)"',
            r'"url":\s*"(.*?\.m3u8[^"]*)"',
            r'"source":\s*"(.*?\.m3u8[^"]*)"',
            r'(https?://[^\s"\']+\.m3u8[^\s"\']*)',
        ]:
            mm = re.search(pat, data)
            if mm:
                return mm.group(1).replace('\\/', '/').replace('\\', '')
        return None

    except Exception:
        return None


def test_items(items, base_domain):
    working = []
    for i, item in enumerate(items):
        print(f"  {i+1:2d}. {item['name']}...", end=" ", flush=True)
        url = get_m3u8(item['id'], base_domain)
        if url:
            print(f"{GREEN}âœ“{RESET}")
            item['url'] = url
            working.append(item)
        else:
            print("âœ—")
    return working


def extinf_line(item, group):
    """#EXTINF satÄ±rÄ± yaz (logo varsa tvg-logo ekle)."""
    logo_attr = f' tvg-logo="{item["logo"]}"' if item.get("logo") else ''
    return (
        f'#EXTINF:-1 tvg-id="{item["id"]}" tvg-name="{item["name"]}"'
        f'{logo_attr} group-title="{group}",{item["name"]}\n'
    )


def build_m3u(working_matches, working_channels, base_domain):
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n\n")

        # â”€â”€ MaÃ§lar
        f.write("# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        f.write("#  CANLI MAÃ‡LAR\n")
        f.write("# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")

        for m in working_matches:
            f.write(extinf_line(m, m['group']))
            if m.get('home_logo'):
                f.write(f'# logo-home: {m["home_logo"]}\n')
            if m.get('away_logo'):
                f.write(f'# logo-away: {m["away_logo"]}\n')
            f.write(f'#EXTVLCOPT:http-referrer={base_domain}\n')
            f.write(f'#EXTVLCOPT:http-user-agent={headers["User-Agent"]}\n')
            f.write(m['url'] + "\n\n")

        # â”€â”€ TV KanallarÄ±
        f.write("# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        f.write("#  TV KANALLARI\n")
        f.write("# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")

        for ch in working_channels:
            f.write(extinf_line(ch, "TV KanallarÄ±"))
            f.write(f'#EXTVLCOPT:http-referrer={base_domain}\n')
            f.write(f'#EXTVLCOPT:http-user-agent={headers["User-Agent"]}\n')
            f.write(ch['url'] + "\n\n")

    print(f"\n{GREEN}[âœ“] {OUTPUT_FILE} oluÅŸturuldu.{RESET}")
    print(f"    âš½ MaÃ§    : {len(working_matches)}")
    print(f"    ğŸ“º Kanal  : {len(working_channels)}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print(f"\n{GREEN}AtomSporTV â€“ MaÃ§ + TV KanallarÄ± M3U OluÅŸturucu{RESET}")
    print("=" * 60)

    print("\n[1] Ana domain bulunuyor...")
    base_domain = get_base_domain()

    print("\n[2] GÃ¼nÃ¼n maÃ§larÄ± Ã§ekiliyor...")
    matches = get_matches()

    print(f"\n[3] {len(matches)} maÃ§Ä±n stream linki test ediliyor...")
    working_matches = test_items(matches, base_domain)

    print(f"\n[4] {len(TV_CHANNELS)} TV kanalÄ± test ediliyor...")
    tv_items = [{'id': c[0], 'name': c[1], 'logo': c[2]} for c in TV_CHANNELS]
    working_channels = test_items(tv_items, base_domain)

    print("\n[5] M3U dosyasÄ± yazÄ±lÄ±yor...")
    build_m3u(working_matches, working_channels, base_domain)

    print("\n" + "=" * 60)
    if working_matches:
        print(f"\n{YELLOW}âš½ Ã‡ALIÅAN MAÃ‡LAR:{RESET}")
        for m in working_matches:
            print(f"   âœ“ {m['name']}")
    if working_channels:
        print(f"\n{YELLOW}ğŸ“º Ã‡ALIÅAN TV KANALLARI:{RESET}")
        for ch in working_channels:
            print(f"   âœ“ {ch['name']}")

    print("\n" + "=" * 60)
    print("GitHub:")
    print(f"  git add {OUTPUT_FILE}")
    print('  git commit -m "GÃ¼nlÃ¼k M3U gÃ¼ncelleme"')
    print("  git push")


if __name__ == "__main__":
    main()
